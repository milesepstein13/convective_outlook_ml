{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cea6da4-bcd8-4452-9be8-3b5ff7d02f2f",
   "metadata": {},
   "source": [
    "This notebook will prepare the input datasets (by loading from era5) and save them to /work/milesep/convective_outlook_ml (2TB limit)\n",
    "This should work without ever loading the data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4042e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba40831-d19a-4580-b653-7a34285af063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying sizes and thinnings\n",
    "\n",
    "lat_dict = {\n",
    "    'full': slice(50, 25),\n",
    "    'small': slice(45, 30),\n",
    "    'slgt_small': slice(50, 25)\n",
    "}\n",
    "\n",
    "lon_dict = {\n",
    "    'full': slice(360-125, 360-66),\n",
    "    'small': slice(360-105, 360-85),\n",
    "    'slgt_small': slice(360-125, 360-66)\n",
    "}\n",
    "\n",
    "levels_dict = {\n",
    "    'full': [925, 850, 700, 500, 300],\n",
    "    'small': [925, 850, 700, 500, 300],\n",
    "    'slgt_small': [925, 850, 700, 500, 300]\n",
    "}\n",
    "\n",
    "time_thin_dict = {\n",
    "    'full': 1,\n",
    "    'small': 6,\n",
    "    'slgt_small': 6\n",
    "}\n",
    "\n",
    "space_thin_dict = {\n",
    "    'full': 1,\n",
    "    'small': 4,\n",
    "    'slgt_small': 4\n",
    "}\n",
    "\n",
    "risk_level_dict = {\n",
    "    'full': ['MDT', 'HIGH'],\n",
    "    'small': ['MDT', 'HIGH'],\n",
    "    'slgt_small': ['SLGT', 'ENH', 'MDT', 'HIGH']\n",
    "}\n",
    "\n",
    "pressure_var_dict = {\n",
    "    'full': [\"geopotential\", \"potential_vorticity\", \"specific_humidity\", \"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"vertical_velocity\"],\n",
    "    'small': [\"geopotential\", \"potential_vorticity\", \"specific_humidity\", \"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"vertical_velocity\"],\n",
    "    'slgt_small': [\"geopotential\", \"potential_vorticity\", \"specific_humidity\", \"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"vertical_velocity\"]\n",
    "}\n",
    "\n",
    "surface_var_dict = {\n",
    "    'full': [\"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"2m_dewpoint_temperature\", \"2m_temperature\", \"geopotential_at_surface\", \"toa_incident_solar_radiation\"],\n",
    "    'small': [\"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"2m_dewpoint_temperature\", \"2m_temperature\", \"geopotential_at_surface\", \"toa_incident_solar_radiation\"],\n",
    "    'slgt_small': [\"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"2m_dewpoint_temperature\", \"2m_temperature\", \"geopotential_at_surface\", \"toa_incident_solar_radiation\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f787e0-a3aa-49bf-86d3-3f021e60b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = 'slgt_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a4f9e5-ab22-4ba4-966d-0713c8ce5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = xr.open_zarr(\n",
    "    'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3',\n",
    "    chunks=None,\n",
    "    storage_options=dict(token='anon'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86410e6a-0f47-4dd9-bcac-86880910121e",
   "metadata": {},
   "source": [
    "Select just desired days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e924c2-1aea-4dc0-b071-ebb139d926ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pph = xr.load_dataset('data/raw_data/labelled_pph.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15b6a6b-9b90-4bca-a6aa-5b5d4833fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = ['200204250000', '200208300000', '200304150000', '200304160000', '200306250000', '200307270000', '200307280000', '200312280000', '200404140000', '200408090000', '200905280000', '201105210000', '202005240000', '200510240000']\n",
    "dates_of_interest = pph['time'][pph['MAX_CAT'].isin(risk_level_dict[detail])]\n",
    "dates_of_interest = dates_of_interest[dates_of_interest > '200203310000']\n",
    "dates_of_interest = dates_of_interest[~(dates_of_interest.isin(missing_dates))]\n",
    "selected_days = pd.to_datetime(dates_of_interest.values, format=\"%Y%m%d%H%M\").normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ddcc832-95da-480f-a22e-2e1299c2de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_days = full_ds.time.dt.floor('D')\n",
    "full_ds = full_ds.sel(time=full_ds.time[np.isin(time_days, selected_days)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8f839",
   "metadata": {},
   "source": [
    "Select lat/lon domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73f223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select just US\n",
    "ds = full_ds.sel(latitude = lat_dict[detail], longitude = lon_dict[detail])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7b8591-dcc9-4daf-8623-6d13593a7861",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.chunk({'time': 240})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22942c3e",
   "metadata": {},
   "source": [
    "Select just MDT+ (or SLGT+ days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37e7800-9950-4f54-b312-9f0005751f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n",
      "/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explicitly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# split time into date and TOD\n",
    "ds = ds.assign_coords(\n",
    "    day=ds.time.dt.floor('D'),\n",
    "    tod=ds.time.dt.hour\n",
    ")\n",
    "ds = ds.set_index(time=['day', 'tod']).unstack('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbaeee7-2b17-43f0-a087-343e4d29cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.thin({'latitude': space_thin_dict[detail], 'longitude': space_thin_dict[detail], 'tod': time_thin_dict[detail]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d12ab8",
   "metadata": {},
   "source": [
    "Select just desired variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "899e9699-c10a-4de4-b6cd-6c7a9f2d089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset each desired pressure-level variable at desired levels\n",
    "ds_pl = xr.Dataset()\n",
    "for var in pressure_var_dict[detail]:\n",
    "    if var in ds:\n",
    "        ds_pl[var] = ds[var].sel(level=levels_dict[detail])\n",
    "\n",
    "ds_sfc = ds[surface_var_dict[detail]]\n",
    "\n",
    "ds_final = xr.merge([ds_pl, ds_sfc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416b64-f41c-4998-99b1-5ad15e720830",
   "metadata": {},
   "source": [
    "Save inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72010394-33f6-437a-8420-162e3ceeae27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_final.to_zarr(\"/glade/work/milesep/convective_outlook_ml/inputs_raw_\" + detail + \".zarr\", mode=\"w\", consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced97fd",
   "metadata": {},
   "source": [
    "To estimate size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13224049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated uncompressed size: 4.17 GB\n"
     ]
    }
   ],
   "source": [
    "def estimate_dataset_size_bytes(ds):\n",
    "    total_bytes = 0\n",
    "    for var in ds.data_vars.values():\n",
    "        if var.chunks is not None:\n",
    "            total_bytes += var.nbytes\n",
    "        else:\n",
    "            # Use 64-bit integers to avoid overflow\n",
    "            n_elements = np.prod(var.shape, dtype=np.int64)\n",
    "            dtype_size = np.dtype(var.dtype).itemsize\n",
    "            total_bytes += int(n_elements * dtype_size)\n",
    "    return total_bytes\n",
    "\n",
    "\n",
    "size_bytes = estimate_dataset_size_bytes(ds_final)\n",
    "print(f\"Estimated uncompressed size: {size_bytes / 1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlco]",
   "language": "python",
   "name": "conda-env-mlco-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
