{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321bbc5e-a389-4094-b7c5-120f81992f71",
   "metadata": {},
   "source": [
    "This file will prepare the test and training target variable datasets (by selecting from contingencies and pph datasets in data/raw_data) and save in data/processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723c9dc3-e9a4-45b4-9a9b-b2bbd9dd6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f082549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "contingencies = xr.load_dataset('data/raw_data/contingency_regions.nc')\n",
    "pph = xr.load_dataset('data/raw_data/labelled_pph.nc')\n",
    "\n",
    "# Define hazard names in order\n",
    "hazards = ['All Hazard', 'Wind', 'Hail', 'Tornado']\n",
    "\n",
    "# Create dictionary for shifts from pph\n",
    "east_keys = ['E_SH_NUM', 'E_SH_W_NUM', 'E_SH_H_NUM', 'E_SH_T_NUM']\n",
    "north_keys = ['N_SH_NUM', 'N_SH_W_NUM', 'N_SH_H_NUM', 'N_SH_T_NUM']\n",
    "\n",
    "# Select valid dates\n",
    "missing_dates = [\n",
    "    '200204250000', '200208300000', '200304150000', '200304160000',\n",
    "    '200306250000', '200307270000', '200307280000', '200312280000',\n",
    "    '200404140000', '200408090000', '200905280000', '201105210000',\n",
    "    '202005240000', '200510240000'\n",
    "]\n",
    "\n",
    "dates_of_interest = pph['time'].where(pph['MAX_CAT'].isin(['MDT', 'HIGH']), drop=True)\n",
    "dates_of_interest = dates_of_interest.where(dates_of_interest > '200203310000', drop=True)\n",
    "dates_of_interest = dates_of_interest.where(~dates_of_interest.isin(missing_dates), drop=True)\n",
    "\n",
    "\n",
    "# Compute bias per hazard\n",
    "def compute_bias(hazard):\n",
    "    da = contingencies.sel(hazard=hazard, region='CONUS')\n",
    "    bias = (da['a'] + da['b']) / (da['a'] + da['c'])\n",
    "    bias = da['b'] - da['c']\n",
    "    return bias.sel(time=dates_of_interest)\n",
    "\n",
    "\n",
    "bias = xr.concat(\n",
    "    [compute_bias(h) for h in hazards], dim='hazard'\n",
    ").assign_coords(hazard=hazards)\n",
    "\n",
    "# Collect and select shifts\n",
    "east_shift = xr.concat(\n",
    "    [pph[k].sel(time=dates_of_interest) for k in east_keys], dim='hazard'\n",
    ").assign_coords(hazard=hazards)\n",
    "\n",
    "north_shift = xr.concat(\n",
    "    [pph[k].sel(time=dates_of_interest) for k in north_keys], dim='hazard'\n",
    ").assign_coords(hazard=hazards)\n",
    "\n",
    "# Combine into Dataset\n",
    "target_ds = xr.Dataset({\n",
    "    'bias': bias,\n",
    "    'east_shift': east_shift,\n",
    "    'north_shift': north_shift\n",
    "})\n",
    "\n",
    "target_ds = target_ds.drop_vars([v for v in target_ds.coords if v not in ['time', 'hazard']])\n",
    "target_ds['time'] = pd.to_datetime(target_ds['time'].values, format='%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c690769",
   "metadata": {},
   "source": [
    "View full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41087a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_rolling_avg(ds, title='Hazard Metrics', save_path=None, rolling_days=365):\n",
    "    \"\"\"\n",
    "    Plot scatter, line of best fit, and 1-year running average for each variable and hazard in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        Dataset with coordinates 'time' and 'hazard', and variables like 'bias', 'east_shift', 'north_shift'.\n",
    "    title : str\n",
    "        Title for the figure.\n",
    "    save_path : str or None\n",
    "        Path to save the figure. If None, the plot will be shown instead.\n",
    "    rolling_days : int\n",
    "        Number of days to use for the rolling average window.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = ds.copy()\n",
    "    variables = ['bias', 'east_shift', 'north_shift']\n",
    "    hazards = ds.hazard.values\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 10), sharex=True)\n",
    "\n",
    "    for row, var in enumerate(variables):\n",
    "        for col, hazard in enumerate(hazards):\n",
    "            ax = axes[row, col]\n",
    "            da = ds[var].sel(hazard=hazard)\n",
    "\n",
    "            # Convert to pandas for rolling and fitting\n",
    "            df = da.to_pandas().dropna()\n",
    "            rolling_mean = df.rolling(f'{rolling_days}D').mean()\n",
    "\n",
    "            # Time conversion for regression (float days since start)\n",
    "            x = (df.index - df.index[0]).total_seconds() / (24 * 3600)\n",
    "            y = df.values\n",
    "\n",
    "            if len(x) >= 2:\n",
    "                # Fit line of best fit (1st-degree poly)\n",
    "                coeffs = np.polyfit(x, y, deg=1)\n",
    "                trend = np.poly1d(coeffs)\n",
    "\n",
    "                # Create fitted line over the full date range\n",
    "                ax.plot(df.index, trend(x), color='red', linestyle='--', label='Linear Fit')\n",
    "\n",
    "            # Plot raw data and rolling average\n",
    "            ax.scatter(df.index, y, s=10, alpha=0.6, label='Raw')\n",
    "            ax.plot(df.index, rolling_mean.values, color='black', label='1-Year Avg')\n",
    "\n",
    "            ax.set_title(f'{var.replace(\"_\", \" \").title()} - {hazard}')\n",
    "            if row == 2:\n",
    "                ax.set_xlabel('Date')\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(var.replace(\"_\", \" \").title())\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # Global legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509b462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_rolling_avg(target_ds, title='Raw Targets with 1-Year Rolling Average', save_path='figs/raw_full_targets.png', rolling_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2d8ff",
   "metadata": {},
   "source": [
    "Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a747f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_ds = target_ds.sel(time=slice('2002-01-01', '2019-12-31'))\n",
    "target_test_ds = target_ds.sel(time=slice('2020-01-01', '2024-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660bbacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_rolling_avg(target_train_ds, title='Training Targets with 1-Year Rolling Average', save_path='figs/raw_training_targets.png', rolling_days=365)\n",
    "plot_metrics_with_rolling_avg(target_test_ds, title='Test Targets with 1-Year Rolling Average', save_path='figs/raw_test_targets.png', rolling_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61ea2d",
   "metadata": {},
   "source": [
    "Detrend training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c928ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_train_ds = target_train_ds.copy()\n",
    "\n",
    "for var in ['bias', 'east_shift', 'north_shift']:\n",
    "    for hazard in target_train_ds.hazard.values:\n",
    "        da = target_train_ds[var].sel(hazard=hazard)\n",
    "        times = da['time']\n",
    "\n",
    "        # Convert time to numeric (e.g., float days since start)\n",
    "        t_numeric = (times - times[0]) / np.timedelta64(1, 'D')\n",
    "        t_numeric = t_numeric.astype(float)\n",
    "\n",
    "        y = da.values\n",
    "\n",
    "        # Fit linear trend: y = m * t + b\n",
    "        m, b = np.polyfit(t_numeric, y, 1)\n",
    "        trend = m * t_numeric + b\n",
    "\n",
    "        # Get value of trend at last training time\n",
    "        t_last = ((times[-1] - times[0]) / np.timedelta64(1, 'D')).astype(float)\n",
    "        offset = m * t_last + b\n",
    "\n",
    "        # Subtract trend and add offset to anchor at end of training\n",
    "        adjusted = y - trend + offset\n",
    "\n",
    "        # Create a new DataArray with adjusted values\n",
    "        adjusted_da = da.copy()\n",
    "        adjusted_da.loc[dict(time=times)] = adjusted\n",
    "\n",
    "        # Assign it back to the detrended dataset\n",
    "        detrended_train_ds[var].loc[dict(hazard=hazard)] = adjusted_da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d289e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_rolling_avg(detrended_train_ds, title='Detrended Training Targets with 1-Year Rolling Average', save_path='figs/detrended_training_targets.png', rolling_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683734bf",
   "metadata": {},
   "source": [
    "Standardize all datasets by test mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d17a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_ds = detrended_train_ds.copy()\n",
    "standardized_test_ds = target_test_ds.copy()\n",
    "\n",
    "train_stats = {}\n",
    "\n",
    "for var in ['bias', 'east_shift', 'north_shift']:\n",
    "    train_stats[var] = {}\n",
    "\n",
    "    for hazard in detrended_train_ds.hazard.values:\n",
    "        # Select the training values\n",
    "        train_values = detrended_train_ds[var].sel(hazard=hazard).values\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean = np.mean(train_values)\n",
    "        std = np.std(train_values)\n",
    "\n",
    "        # Save stats for later (optional)\n",
    "        train_stats[var][hazard.item()] = {'mean': mean, 'std': std}\n",
    "\n",
    "        # Standardize both datasets\n",
    "        standardized_train_ds[var].loc[dict(hazard=hazard)] = (\n",
    "            detrended_train_ds[var].sel(hazard=hazard) - mean\n",
    "        ) / std\n",
    "\n",
    "        standardized_test_ds[var].loc[dict(hazard=hazard)] = (\n",
    "            target_test_ds[var].sel(hazard=hazard) - mean\n",
    "        ) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d978c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = xr.DataArray(\n",
    "    [[train_stats[var][hazard]['mean'] for hazard in standardized_train_ds.hazard.values] for var in train_stats],\n",
    "    coords=[list(train_stats.keys()), standardized_train_ds.hazard.values],\n",
    "    dims=[\"variable\", \"hazard\"]\n",
    ")\n",
    "\n",
    "stds = xr.DataArray(\n",
    "    [[train_stats[var][hazard]['std'] for hazard in standardized_train_ds.hazard.values] for var in train_stats],\n",
    "    coords=[list(train_stats.keys()), standardized_train_ds.hazard.values],\n",
    "    dims=[\"variable\", \"hazard\"]\n",
    ")\n",
    "\n",
    "standardized_train_ds[\"train_mean\"] = means\n",
    "standardized_train_ds[\"train_std\"] = stds\n",
    "\n",
    "\n",
    "standardized_test_ds[\"train_mean\"] = means\n",
    "standardized_test_ds[\"train_std\"] = stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3600e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_rolling_avg(standardized_train_ds, title='Standardized Training Targets with 1-Year Rolling Average', save_path='figs/standardized_training_targets.png', rolling_days=365)\n",
    "plot_metrics_with_rolling_avg(standardized_test_ds, title='Standardized Test Targets with 1-Year Rolling Average', save_path='figs/standardized_test_targets.png', rolling_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177575e",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "837981b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_ds.to_netcdf('data/processed_data/train_targets.nc')\n",
    "standardized_test_ds.to_netcdf('data/processed_data/test_targets.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlco]",
   "language": "python",
   "name": "conda-env-mlco-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
