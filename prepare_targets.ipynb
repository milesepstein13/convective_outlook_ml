{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321bbc5e-a389-4094-b7c5-120f81992f71",
   "metadata": {},
   "source": [
    "This file will prepare the test and training target variable datasets (by selecting from contingencies and pph datasets in data/raw_data) and save in data/processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "723c9dc3-e9a4-45b4-9a9b-b2bbd9dd6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f082549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xarray\\core\\concat.py:532: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n",
      "C:\\Users\\miles\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xarray\\core\\concat.py:532: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n",
      "C:\\Users\\miles\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xarray\\core\\concat.py:532: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "contingencies = xr.load_dataset('data/raw_data/contingency_regions.nc')\n",
    "pph = xr.load_dataset('data/raw_data/labelled_pph.nc')\n",
    "\n",
    "# Define hazard names in order\n",
    "hazards = ['All Hazard', 'Wind', 'Hail', 'Tornado']\n",
    "\n",
    "# Create dictionary for shifts from pph\n",
    "east_keys = ['E_SH_NUM', 'E_SH_W_NUM', 'E_SH_H_NUM', 'E_SH_T_NUM']\n",
    "north_keys = ['N_SH_NUM', 'N_SH_W_NUM', 'N_SH_H_NUM', 'N_SH_T_NUM']\n",
    "\n",
    "# Select valid dates\n",
    "missing_dates = [\n",
    "    '200204250000', '200208300000', '200304150000', '200304160000',\n",
    "    '200306250000', '200307270000', '200307280000', '200312280000',\n",
    "    '200404140000', '200408090000', '200905280000', '201105210000',\n",
    "    '202005240000', '200510240000'\n",
    "]\n",
    "\n",
    "dates_of_interest = pph['time'].where(pph['MAX_CAT'].isin(['MDT', 'HIGH']), drop=True)\n",
    "dates_of_interest = dates_of_interest.where(dates_of_interest > '200203310000', drop=True)\n",
    "dates_of_interest = dates_of_interest.where(~dates_of_interest.isin(missing_dates), drop=True)\n",
    "\n",
    "# Compute bias per hazard\n",
    "def compute_bias(hazard):\n",
    "    da = contingencies.sel(hazard=hazard, region='CONUS')\n",
    "    bias = (da['a'] + da['b']) / (da['a'] + da['c'])\n",
    "    bias = da['b'] - da['c']\n",
    "    return bias.sel(time=dates_of_interest)\n",
    "\n",
    "bias = xr.concat(\n",
    "    [compute_bias(h) for h in hazards], dim='hazard'\n",
    ").assign_coords(hazard=hazards)\n",
    "\n",
    "# Collect and select shifts\n",
    "east_shift = xr.concat(\n",
    "    [pph[k].sel(time=dates_of_interest) for k in east_keys], dim='hazard'\n",
    ").assign_coords(hazard=hazards)\n",
    "\n",
    "north_shift = xr.concat(\n",
    "    [pph[k].sel(time=dates_of_interest) for k in north_keys], dim='hazard'\n",
    ").assign_coords(hazard=hazards)\n",
    "\n",
    "# Combine into Dataset\n",
    "target_ds = xr.Dataset({\n",
    "    'bias': bias,\n",
    "    'east_shift': east_shift,\n",
    "    'north_shift': north_shift\n",
    "})\n",
    "\n",
    "target_ds = target_ds.drop_vars([v for v in target_ds.coords if v not in ['time', 'hazard']])\n",
    "target_ds['time'] = pd.to_datetime(target_ds['time'].values, format='%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c690769",
   "metadata": {},
   "source": [
    "View full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41087a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_rolling_avg(ds, title='Hazard Metrics', save_path=None, rolling_days=365):\n",
    "    \"\"\"\n",
    "    Plot scatter, line of best fit, and 1-year running average for each variable and hazard in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        Dataset with coordinates 'time' and 'hazard', and variables like 'bias', 'east_shift', 'north_shift'.\n",
    "    title : str\n",
    "        Title for the figure.\n",
    "    save_path : str or None\n",
    "        Path to save the figure. If None, the plot will be shown instead.\n",
    "    rolling_days : int\n",
    "        Number of days to use for the rolling average window.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = ds.copy()\n",
    "    variables = ['bias', 'east_shift', 'north_shift']\n",
    "    hazards = ds.hazard.values\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 10), sharex=True)\n",
    "\n",
    "    for row, var in enumerate(variables):\n",
    "        for col, hazard in enumerate(hazards):\n",
    "            ax = axes[row, col]\n",
    "            da = ds[var].sel(hazard=hazard)\n",
    "\n",
    "            # Convert to pandas for rolling and fitting\n",
    "            df = da.to_pandas().dropna()\n",
    "            rolling_mean = df.rolling(f'{rolling_days}D').mean()\n",
    "\n",
    "            # Time conversion for regression (float days since start)\n",
    "            x = (df.index - df.index[0]).total_seconds() / (24 * 3600)\n",
    "            y = df.values\n",
    "\n",
    "            if len(x) >= 2:\n",
    "                # Fit line of best fit (1st-degree poly)\n",
    "                coeffs = np.polyfit(x, y, deg=1)\n",
    "                trend = np.poly1d(coeffs)\n",
    "\n",
    "                # Create fitted line over the full date range\n",
    "                ax.plot(df.index, trend(x), color='red', linestyle='--', label='Linear Fit')\n",
    "\n",
    "            # Plot raw data and rolling average\n",
    "            ax.scatter(df.index, y, s=10, alpha=0.6, label='Raw')\n",
    "            ax.plot(df.index, rolling_mean.values, color='black', label='1-Year Avg')\n",
    "\n",
    "            ax.set_title(f'{var.replace(\"_\", \" \").title()} - {hazard}')\n",
    "            if row == 2:\n",
    "                ax.set_xlabel('Date')\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(var.replace(\"_\", \" \").title())\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # Global legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "509b462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_rolling_avg(target_ds, title='Raw Targets with 1-Year Rolling Average', save_path='figs/full_raw_targets.png', rolling_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2d8ff",
   "metadata": {},
   "source": [
    "Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a747f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_ds = target_ds.sel(time=slice('2002-01-01', '2019-12-31'))\n",
    "target_test_ds = target_ds.sel(time=slice('2020-01-01', '2024-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "660bbacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_rolling_avg(target_train_ds, title='Training Targets with 1-Year Rolling Average', save_path='figs/training_raw_targets.png', rolling_days=365)\n",
    "plot_metrics_with_rolling_avg(target_test_ds, title='Test Targets with 1-Year Rolling Average', save_path='figs/test_raw_targets.png', rolling_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61ea2d",
   "metadata": {},
   "source": [
    "Detrend training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6c928ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_train_ds = target_train_ds.copy()\n",
    "\n",
    "for var in ['bias', 'east_shift', 'north_shift']:\n",
    "    for hazard in target_train_ds.hazard.values:\n",
    "        da = target_train_ds[var].sel(hazard=hazard)\n",
    "        times = da['time']\n",
    "        \n",
    "        # Convert time to numeric (e.g., float days since start)\n",
    "        t_numeric = (times - times[0]) / np.timedelta64(1, 'D')\n",
    "        t_numeric = t_numeric.astype(float)\n",
    "        \n",
    "        y = da.values\n",
    "        \n",
    "        # Fit linear trend: y = m * t + b\n",
    "        m, b = np.polyfit(t_numeric, y, 1)\n",
    "        trend = m * t_numeric + b\n",
    "        \n",
    "        # Get value of trend at last training time\n",
    "        t_last = ((times[-1] - times[0]) / np.timedelta64(1, 'D')).astype(float)\n",
    "        offset = m * t_last + b\n",
    "        \n",
    "        # Subtract trend and add offset to anchor at end of training\n",
    "        adjusted = y - trend + offset\n",
    "\n",
    "        # Create a new DataArray with adjusted values\n",
    "        adjusted_da = da.copy()\n",
    "        adjusted_da.loc[dict(time=times)] = adjusted\n",
    "\n",
    "        # Assign it back to the detrended dataset\n",
    "        detrended_train_ds[var].loc[dict(hazard=hazard)] = adjusted_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d289e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_rolling_avg(detrended_train_ds, title='Detrended Training Targets with 1-Year Rolling Average', save_path='figs/training_detrended_targets.png', rolling_days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683734bf",
   "metadata": {},
   "source": [
    "Standardize all datasets by test mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d17a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f177575e",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837981b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
