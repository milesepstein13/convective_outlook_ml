{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c87307c-c45a-40cd-a668-fb01cbb6b91c",
   "metadata": {},
   "source": [
    "This notebook will compare model architecture, hyperparameters, inputs, and training/validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7640bc2-c69c-4959-9d04-e4bf91e1fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from src.crossval import run_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf77751",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/glade/work/milesep/convective_outlook_ml\"\n",
    "target_dir = \"data/processed_data\"\n",
    "stats_dir = \"data/processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "574e71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression small\n",
      "\n",
      "Fold 0:\n",
      "Loading data\n",
      "standardizing data\n",
      "setting up training\n",
      "Fold already trained (latest epoch = 49 ≥ target = 49) — skipping.\n",
      "\n",
      "Fold 1:\n",
      "Loading data\n",
      "standardizing data\n",
      "setting up training\n",
      "Resuming training from epoch 45/50\n",
      "  Epoch 46/50 — Train Loss: 0.1932, Val Loss: 2.2080\n",
      "  Epoch 47/50 — Train Loss: 0.2055, Val Loss: 2.4079\n",
      "  Epoch 48/50 — Train Loss: 0.1926, Val Loss: 2.2521\n",
      "  Epoch 49/50 — Train Loss: 0.2128, Val Loss: 2.6889\n",
      "  Epoch 50/50 — Train Loss: 0.2240, Val Loss: 2.3162\n",
      "\n",
      "Fold 2:\n",
      "Loading data\n",
      "standardizing data\n",
      "setting up training\n",
      "Resuming training from epoch 20/50\n",
      "  Epoch 21/50 — Train Loss: 1.1373, Val Loss: 2.1015\n",
      "  Epoch 22/50 — Train Loss: 0.8787, Val Loss: 2.2015\n",
      "  Epoch 23/50 — Train Loss: 0.7763, Val Loss: 2.5232\n",
      "  Epoch 24/50 — Train Loss: 0.7550, Val Loss: 1.7629\n",
      "  Epoch 25/50 — Train Loss: 0.5262, Val Loss: 1.7821\n",
      "  Epoch 26/50 — Train Loss: 0.4728, Val Loss: 1.9116\n",
      "  Epoch 27/50 — Train Loss: 0.5226, Val Loss: 1.8122\n",
      "  Epoch 28/50 — Train Loss: 0.5195, Val Loss: 1.6794\n",
      "  Epoch 29/50 — Train Loss: 0.6227, Val Loss: 2.0199\n",
      "  Epoch 30/50 — Train Loss: 0.9797, Val Loss: 2.8363\n",
      "  Epoch 31/50 — Train Loss: 1.1769, Val Loss: 1.9209\n",
      "  Epoch 32/50 — Train Loss: 0.9457, Val Loss: 2.3468\n",
      "  Epoch 33/50 — Train Loss: 1.3189, Val Loss: 4.4375\n",
      "  Epoch 34/50 — Train Loss: 1.4803, Val Loss: 2.1295\n",
      "  Epoch 35/50 — Train Loss: 1.2674, Val Loss: 2.3259\n",
      "  Epoch 36/50 — Train Loss: 1.1045, Val Loss: 2.2476\n",
      "  Epoch 37/50 — Train Loss: 0.8912, Val Loss: 1.8314\n",
      "  Epoch 38/50 — Train Loss: 0.6773, Val Loss: 2.1029\n",
      "  Epoch 39/50 — Train Loss: 0.7851, Val Loss: 1.7471\n",
      "  Epoch 40/50 — Train Loss: 0.6894, Val Loss: 1.7632\n",
      "  Epoch 41/50 — Train Loss: 0.6121, Val Loss: 1.6965\n",
      "  Epoch 42/50 — Train Loss: 0.6314, Val Loss: 2.1995\n",
      "  Epoch 43/50 — Train Loss: 0.8602, Val Loss: 1.6560\n",
      "  Epoch 44/50 — Train Loss: 0.6167, Val Loss: 2.0248\n",
      "  Epoch 45/50 — Train Loss: 0.4991, Val Loss: 1.5978\n",
      "  Epoch 46/50 — Train Loss: 0.4822, Val Loss: 1.6870\n",
      "  Epoch 47/50 — Train Loss: 0.5137, Val Loss: 1.6175\n",
      "  Epoch 48/50 — Train Loss: 0.4942, Val Loss: 2.2190\n",
      "  Epoch 49/50 — Train Loss: 0.5689, Val Loss: 1.7287\n",
      "  Epoch 50/50 — Train Loss: 0.5365, Val Loss: 2.0948\n",
      "\n",
      "Fold 3:\n",
      "Loading data\n",
      "standardizing data\n",
      "setting up training\n",
      "Resuming training from epoch 20/50\n",
      "  Epoch 21/50 — Train Loss: 1.2084, Val Loss: 1.5991\n",
      "  Epoch 22/50 — Train Loss: 1.0119, Val Loss: 1.5418\n",
      "  Epoch 23/50 — Train Loss: 1.0341, Val Loss: 1.4756\n",
      "  Epoch 24/50 — Train Loss: 1.0268, Val Loss: 1.6321\n",
      "  Epoch 25/50 — Train Loss: 1.0984, Val Loss: 1.3827\n",
      "  Epoch 26/50 — Train Loss: 0.8759, Val Loss: 1.5824\n",
      "  Epoch 27/50 — Train Loss: 1.0020, Val Loss: 1.3165\n",
      "  Epoch 28/50 — Train Loss: 0.6755, Val Loss: 1.3126\n",
      "  Epoch 29/50 — Train Loss: 0.8822, Val Loss: 1.1812\n",
      "  Epoch 30/50 — Train Loss: 0.9198, Val Loss: 1.1365\n",
      "  Epoch 31/50 — Train Loss: 0.7697, Val Loss: 1.4563\n",
      "  Epoch 32/50 — Train Loss: 0.7444, Val Loss: 1.1482\n",
      "  Epoch 33/50 — Train Loss: 0.6368, Val Loss: 1.2355\n",
      "  Epoch 34/50 — Train Loss: 0.6323, Val Loss: 1.0769\n",
      "  Epoch 35/50 — Train Loss: 0.5590, Val Loss: 1.2488\n",
      "  Epoch 36/50 — Train Loss: 0.6154, Val Loss: 1.1560\n",
      "  Epoch 37/50 — Train Loss: 0.5088, Val Loss: 1.1056\n",
      "  Epoch 38/50 — Train Loss: 0.4577, Val Loss: 1.2355\n",
      "  Epoch 39/50 — Train Loss: 0.4982, Val Loss: 1.1329\n",
      "  Epoch 40/50 — Train Loss: 0.4491, Val Loss: 1.1775\n",
      "  Epoch 41/50 — Train Loss: 0.5164, Val Loss: 1.1119\n",
      "  Epoch 42/50 — Train Loss: 0.5998, Val Loss: 1.1537\n",
      "  Epoch 43/50 — Train Loss: 0.6585, Val Loss: 1.4184\n",
      "  Epoch 44/50 — Train Loss: 0.7587, Val Loss: 1.2423\n",
      "  Epoch 45/50 — Train Loss: 0.5422, Val Loss: 1.0848\n",
      "  Epoch 46/50 — Train Loss: 0.4450, Val Loss: 1.2384\n",
      "  Epoch 47/50 — Train Loss: 0.4587, Val Loss: 1.3235\n",
      "  Epoch 48/50 — Train Loss: 0.5184, Val Loss: 1.3382\n",
      "  Epoch 49/50 — Train Loss: 0.4827, Val Loss: 1.0901\n",
      "  Epoch 50/50 — Train Loss: 0.3872, Val Loss: 1.2888\n",
      "\n",
      "Fold 4:\n",
      "Loading data\n",
      "standardizing data\n",
      "setting up training\n",
      "Resuming training from epoch 20/50\n",
      "  Epoch 21/50 — Train Loss: 1.1074, Val Loss: 1.6818\n",
      "  Epoch 22/50 — Train Loss: 1.2285, Val Loss: 1.3328\n",
      "  Epoch 23/50 — Train Loss: 1.2351, Val Loss: 1.5600\n",
      "  Epoch 24/50 — Train Loss: 1.1408, Val Loss: 1.2322\n",
      "  Epoch 25/50 — Train Loss: 0.8947, Val Loss: 1.3510\n",
      "  Epoch 26/50 — Train Loss: 0.7736, Val Loss: 1.9200\n",
      "  Epoch 27/50 — Train Loss: 0.8680, Val Loss: 1.2055\n",
      "  Epoch 28/50 — Train Loss: 1.0256, Val Loss: 1.2611\n",
      "  Epoch 29/50 — Train Loss: 0.9838, Val Loss: 1.1068\n",
      "  Epoch 30/50 — Train Loss: 0.6628, Val Loss: 1.2396\n",
      "  Epoch 31/50 — Train Loss: 0.6030, Val Loss: 1.2319\n",
      "  Epoch 32/50 — Train Loss: 0.5796, Val Loss: 0.9342\n",
      "  Epoch 33/50 — Train Loss: 0.5041, Val Loss: 1.0930\n",
      "  Epoch 34/50 — Train Loss: 0.6719, Val Loss: 0.9077\n",
      "  Epoch 35/50 — Train Loss: 0.4963, Val Loss: 1.1118\n",
      "  Epoch 36/50 — Train Loss: 0.5751, Val Loss: 1.1279\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m targets \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_targets.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m stats \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/daily_input_stats_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mrun_crossval\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/convective_outlook_ml/src/crossval.py:109\u001b[0m, in \u001b[0;36mrun_crossval\u001b[0;34m(X, y, stats, model_name, n_splits, batch_size, epochs, optimizer_class, lr, criterion, level, restart)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs):\n\u001b[1;32m    108\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, optimizer, criterion, epoch, writer)\n\u001b[0;32m--> 109\u001b[0m     val_loss, all_preds, all_targets \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# hazards and variables in same order as flatten_target_dataset concatenates them\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     hazards \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll Hazard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHail\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTornado\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/convective_outlook_ml/src/train_loop.py:25\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, loader, criterion, epoch, writer)\u001b[0m\n\u001b[1;32m     23\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/convective_outlook_ml/src/dataset.py:16\u001b[0m, in \u001b[0;36mLazyWeatherDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 16\u001b[0m     day_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mday\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dimensions \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     19\u001b[0m         features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/core/dataset.py:863\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    860\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data):\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/namedarray/daskmanager.py:86\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[0;34m(self, *data, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     83\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/dask/base.py:655\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args\n\u001b[1;32m    649\u001b[0m schedule \u001b[38;5;241m=\u001b[39m get_scheduler(\n\u001b[1;32m    650\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m    651\u001b[0m     collections\u001b[38;5;241m=\u001b[39mcollections,\n\u001b[1;32m    652\u001b[0m     get\u001b[38;5;241m=\u001b[39mget,\n\u001b[1;32m    653\u001b[0m )\n\u001b[0;32m--> 655\u001b[0m dsk \u001b[38;5;241m=\u001b[39m \u001b[43mcollections_to_dsk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m keys, postcomputes \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m collections:\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/dask/base.py:428\u001b[0m, in \u001b[0;36mcollections_to_dsk\u001b[0;34m(collections, optimize_graph, optimizations, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt, val \u001b[38;5;129;01min\u001b[39;00m groups\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    427\u001b[0m     dsk, keys \u001b[38;5;241m=\u001b[39m _extract_graph_and_keys(val)\n\u001b[0;32m--> 428\u001b[0m     dsk \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m opt_inner \u001b[38;5;129;01min\u001b[39;00m optimizations:\n\u001b[1;32m    431\u001b[0m         dsk \u001b[38;5;241m=\u001b[39m opt_inner(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/dask/array/optimization.py:51\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(dsk, keys, fuse_keys, fast_functions, inline_functions_fast_functions, rename_fused_keys, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m dsk \u001b[38;5;241m=\u001b[39m optimize_blockwise(dsk, keys\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[1;32m     50\u001b[0m dsk \u001b[38;5;241m=\u001b[39m fuse_roots(dsk, keys\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[0;32m---> 51\u001b[0m dsk \u001b[38;5;241m=\u001b[39m \u001b[43mdsk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcull\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Perform low-level fusion unless the user has\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# specified False explicitly.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimization.fuse.active\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/dask/highlevelgraph.py:736\u001b[0m, in \u001b[0;36mHighLevelGraph.cull\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m    730\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[layer_name]\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# Let's cull the layer to produce its part of `keys`.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# Note: use .intersection rather than & because the RHS is\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# a collections.abc.Set rather than a real set, and using &\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# would take time proportional to the size of the LHS, which\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;66;03m# if there is no culling can be much bigger than the RHS.\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m output_keys \u001b[38;5;241m=\u001b[39m keys_set\u001b[38;5;241m.\u001b[39mintersection(\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_keys:\n\u001b[1;32m    738\u001b[0m     culled_layer, culled_deps \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mcull(output_keys, all_ext_keys)\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/dask/blockwise.py:484\u001b[0m, in \u001b[0;36mBlockwise.get_output_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput, \u001b[38;5;241m*\u001b[39mp) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_blocks}\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Return all possible output keys (no culling)\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/dask/blockwise.py:484\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput, \u001b[38;5;241m*\u001b[39mp) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_blocks}\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Return all possible output keys (no culling)\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    485\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput, \u001b[38;5;241m*\u001b[39mp)\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_indices]\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    489\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compare all models, save training and validation curves along with corresponding model name, data level, model/training hyperparameters\n",
    "model_names = [\"linear_regression\", \"cnn3d_dropout_0_5\", \"cnn3d_dropout_5_5\", \"cnn3d_dropout_5_0\"]\n",
    "levels = [\"small\", \"small\", \"small\", \"small\", \"small\"]\n",
    "# Don't do linear regression with full dataset--too many parameters\n",
    "for name, level in zip(model_names, levels):\n",
    "    print(name, level)\n",
    "    inputs = xr.open_zarr(f\"{input_dir}/train_inputs_{level}.zarr\")\n",
    "    targets = xr.open_dataset(f\"{target_dir}/train_targets.nc\")\n",
    "    stats = xr.open_dataset(f\"{stats_dir}/daily_input_stats_{level}.nc\")\n",
    "    scores = run_crossval(inputs, targets, stats, name, epochs = 50, level = level, restart = False)\n",
    "    print(f\"{name}: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlco]",
   "language": "python",
   "name": "conda-env-mlco-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
