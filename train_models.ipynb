{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c87307c-c45a-40cd-a668-fb01cbb6b91c",
   "metadata": {},
   "source": [
    "This notebook will compare model architecture, hyperparameters, inputs, and training/validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7640bc2-c69c-4959-9d04-e4bf91e1fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from src.crossval import run_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf77751",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/glade/work/milesep/convective_outlook_ml\"\n",
    "target_dir = \"data/processed_data\"\n",
    "stats_dir = \"data/processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574e71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression small\n",
      "\n",
      "Fold 0:\n",
      "Resuming training from epoch 2/40\n",
      "  Epoch 3/40 — Train Loss: 84.1825, Val Loss: 54.0264\n",
      "  Epoch 4/40 — Train Loss: 82.6561, Val Loss: 22.5831\n",
      "  Epoch 5/40 — Train Loss: 47.8445, Val Loss: 28.3943\n",
      "  Epoch 6/40 — Train Loss: 29.7669, Val Loss: 25.2099\n",
      "  Epoch 7/40 — Train Loss: 26.4385, Val Loss: 8.6109\n",
      "  Epoch 8/40 — Train Loss: 21.0711, Val Loss: 6.6179\n",
      "  Epoch 9/40 — Train Loss: 12.7065, Val Loss: 8.0260\n",
      "  Epoch 10/40 — Train Loss: 8.9884, Val Loss: 6.5138\n",
      "  Epoch 11/40 — Train Loss: 9.1586, Val Loss: 5.1406\n",
      "  Epoch 12/40 — Train Loss: 6.3795, Val Loss: 4.2097\n",
      "  Epoch 13/40 — Train Loss: 5.4790, Val Loss: 3.1361\n",
      "  Epoch 14/40 — Train Loss: 3.8228, Val Loss: 3.0012\n",
      "  Epoch 15/40 — Train Loss: 3.0694, Val Loss: 3.4821\n",
      "  Epoch 16/40 — Train Loss: 2.4696, Val Loss: 2.5187\n",
      "  Epoch 17/40 — Train Loss: 2.2178, Val Loss: 2.6386\n",
      "  Epoch 18/40 — Train Loss: 1.6110, Val Loss: 3.0864\n",
      "  Epoch 19/40 — Train Loss: 1.5925, Val Loss: 2.4180\n",
      "  Epoch 20/40 — Train Loss: 1.4277, Val Loss: 2.2182\n",
      "  Epoch 21/40 — Train Loss: 1.4287, Val Loss: 4.0975\n",
      "  Epoch 22/40 — Train Loss: 1.4538, Val Loss: 2.7389\n",
      "  Epoch 23/40 — Train Loss: 1.2280, Val Loss: 2.0270\n",
      "  Epoch 24/40 — Train Loss: 1.3248, Val Loss: 2.2455\n",
      "  Epoch 25/40 — Train Loss: 1.0382, Val Loss: 5.1395\n",
      "  Epoch 26/40 — Train Loss: 1.2887, Val Loss: 1.9666\n",
      "  Epoch 27/40 — Train Loss: 1.1545, Val Loss: 2.3725\n",
      "  Epoch 28/40 — Train Loss: 1.3644, Val Loss: 2.5722\n",
      "  Epoch 29/40 — Train Loss: 1.2631, Val Loss: 1.9455\n",
      "  Epoch 30/40 — Train Loss: 0.7942, Val Loss: 2.1642\n",
      "  Epoch 31/40 — Train Loss: 0.9631, Val Loss: 1.8784\n",
      "  Epoch 32/40 — Train Loss: 0.7487, Val Loss: 2.3737\n",
      "  Epoch 33/40 — Train Loss: 0.7397, Val Loss: 2.3705\n",
      "  Epoch 34/40 — Train Loss: 0.6617, Val Loss: 2.1393\n",
      "  Epoch 35/40 — Train Loss: 0.6710, Val Loss: 1.7832\n",
      "  Epoch 36/40 — Train Loss: 0.8308, Val Loss: 2.1372\n",
      "  Epoch 37/40 — Train Loss: 0.7568, Val Loss: 5.0894\n",
      "  Epoch 38/40 — Train Loss: 1.6774, Val Loss: 2.5489\n",
      "  Epoch 39/40 — Train Loss: 1.5636, Val Loss: 2.8347\n",
      "  Epoch 40/40 — Train Loss: 1.4449, Val Loss: 1.6793\n",
      "\n",
      "Fold 1:\n",
      "Resuming training from epoch 2/40\n",
      "  Epoch 3/40 — Train Loss: 46.4187, Val Loss: 43.2299\n",
      "  Epoch 4/40 — Train Loss: 49.0654, Val Loss: 46.2443\n",
      "  Epoch 5/40 — Train Loss: 29.2408, Val Loss: 36.3668\n",
      "  Epoch 6/40 — Train Loss: 31.5965, Val Loss: 18.0502\n",
      "  Epoch 7/40 — Train Loss: 24.3875, Val Loss: 15.4682\n",
      "  Epoch 8/40 — Train Loss: 17.5194, Val Loss: 13.6907\n",
      "  Epoch 9/40 — Train Loss: 13.6315, Val Loss: 8.4847\n",
      "  Epoch 10/40 — Train Loss: 10.7955, Val Loss: 9.0948\n",
      "  Epoch 11/40 — Train Loss: 8.4687, Val Loss: 8.4857\n",
      "  Epoch 12/40 — Train Loss: 6.3867, Val Loss: 8.1158\n",
      "  Epoch 13/40 — Train Loss: 5.1057, Val Loss: 7.8938\n",
      "  Epoch 14/40 — Train Loss: 3.8781, Val Loss: 4.5932\n",
      "  Epoch 15/40 — Train Loss: 3.3125, Val Loss: 4.0336\n",
      "  Epoch 16/40 — Train Loss: 2.8461, Val Loss: 6.0504\n",
      "  Epoch 17/40 — Train Loss: 2.2522, Val Loss: 6.5860\n",
      "  Epoch 18/40 — Train Loss: 2.2904, Val Loss: 3.8593\n",
      "  Epoch 19/40 — Train Loss: 1.6714, Val Loss: 4.6945\n",
      "  Epoch 20/40 — Train Loss: 1.1866, Val Loss: 3.1831\n",
      "  Epoch 21/40 — Train Loss: 1.0391, Val Loss: 2.9300\n",
      "  Epoch 22/40 — Train Loss: 1.0450, Val Loss: 3.1027\n",
      "  Epoch 23/40 — Train Loss: 0.7089, Val Loss: 3.2061\n",
      "  Epoch 24/40 — Train Loss: 0.5802, Val Loss: 2.6307\n",
      "  Epoch 25/40 — Train Loss: 0.5165, Val Loss: 2.3977\n",
      "  Epoch 26/40 — Train Loss: 0.4883, Val Loss: 4.4342\n",
      "  Epoch 27/40 — Train Loss: 0.5960, Val Loss: 3.1804\n",
      "  Epoch 28/40 — Train Loss: 0.7733, Val Loss: 3.6893\n",
      "  Epoch 29/40 — Train Loss: 0.8238, Val Loss: 3.1173\n",
      "  Epoch 30/40 — Train Loss: 0.5709, Val Loss: 2.7727\n",
      "  Epoch 31/40 — Train Loss: 0.7632, Val Loss: 4.6860\n",
      "  Epoch 32/40 — Train Loss: 0.8026, Val Loss: 3.2129\n",
      "  Epoch 33/40 — Train Loss: 0.9823, Val Loss: 3.8997\n",
      "  Epoch 34/40 — Train Loss: 0.6577, Val Loss: 3.5447\n",
      "  Epoch 35/40 — Train Loss: 0.6031, Val Loss: 2.6961\n",
      "  Epoch 36/40 — Train Loss: 0.5610, Val Loss: 2.7380\n",
      "  Epoch 37/40 — Train Loss: 0.6277, Val Loss: 3.0684\n",
      "  Epoch 38/40 — Train Loss: 0.6799, Val Loss: 2.8721\n",
      "  Epoch 39/40 — Train Loss: 0.6367, Val Loss: 2.6006\n",
      "  Epoch 40/40 — Train Loss: 0.5597, Val Loss: 3.7493\n",
      "\n",
      "Fold 2:\n",
      "Resuming training from epoch 2/40\n",
      "  Epoch 3/40 — Train Loss: 56.8193, Val Loss: 63.5950\n",
      "  Epoch 4/40 — Train Loss: 52.2034, Val Loss: 17.6157\n",
      "  Epoch 5/40 — Train Loss: 30.4861, Val Loss: 23.5299\n",
      "  Epoch 6/40 — Train Loss: 18.7932, Val Loss: 20.0210\n",
      "  Epoch 7/40 — Train Loss: 15.2598, Val Loss: 8.4408\n",
      "  Epoch 8/40 — Train Loss: 12.3817, Val Loss: 7.9081\n",
      "  Epoch 9/40 — Train Loss: 7.6159, Val Loss: 5.7245\n",
      "  Epoch 10/40 — Train Loss: 6.8604, Val Loss: 5.1033\n",
      "  Epoch 11/40 — Train Loss: 5.4595, Val Loss: 5.7467\n",
      "  Epoch 12/40 — Train Loss: 3.9370, Val Loss: 3.1322\n",
      "  Epoch 13/40 — Train Loss: 2.9979, Val Loss: 3.6082\n",
      "  Epoch 14/40 — Train Loss: 2.4326, Val Loss: 2.6144\n",
      "  Epoch 15/40 — Train Loss: 1.8778, Val Loss: 2.5153\n",
      "  Epoch 16/40 — Train Loss: 1.3678, Val Loss: 1.9242\n",
      "  Epoch 17/40 — Train Loss: 1.1808, Val Loss: 2.0402\n",
      "  Epoch 18/40 — Train Loss: 0.9211, Val Loss: 1.7812\n",
      "  Epoch 19/40 — Train Loss: 0.7309, Val Loss: 1.7688\n",
      "  Epoch 20/40 — Train Loss: 0.6474, Val Loss: 2.1868\n",
      "  Epoch 21/40 — Train Loss: 0.5904, Val Loss: 1.8802\n",
      "  Epoch 22/40 — Train Loss: 0.5715, Val Loss: 1.6155\n",
      "  Epoch 23/40 — Train Loss: 0.4335, Val Loss: 1.6514\n",
      "  Epoch 24/40 — Train Loss: 0.3572, Val Loss: 1.3510\n",
      "  Epoch 25/40 — Train Loss: 0.2913, Val Loss: 1.9883\n",
      "  Epoch 26/40 — Train Loss: 0.2564, Val Loss: 1.5490\n",
      "  Epoch 27/40 — Train Loss: 0.2212, Val Loss: 1.6648\n",
      "  Epoch 28/40 — Train Loss: 0.1875, Val Loss: 1.6508\n",
      "  Epoch 29/40 — Train Loss: 0.1611, Val Loss: 1.5707\n",
      "  Epoch 30/40 — Train Loss: 0.1487, Val Loss: 1.4023\n",
      "  Epoch 31/40 — Train Loss: 0.1482, Val Loss: 1.5187\n",
      "  Epoch 32/40 — Train Loss: 0.1255, Val Loss: 1.3226\n",
      "  Epoch 33/40 — Train Loss: 0.1512, Val Loss: 1.6438\n",
      "  Epoch 34/40 — Train Loss: 0.1192, Val Loss: 1.5360\n",
      "  Epoch 35/40 — Train Loss: 0.0980, Val Loss: 1.6053\n",
      "  Epoch 36/40 — Train Loss: 0.1144, Val Loss: 1.6655\n",
      "  Epoch 37/40 — Train Loss: 0.0995, Val Loss: 1.4709\n",
      "  Epoch 38/40 — Train Loss: 0.0995, Val Loss: 1.5493\n",
      "  Epoch 39/40 — Train Loss: 0.1009, Val Loss: 1.7166\n",
      "  Epoch 40/40 — Train Loss: 0.1032, Val Loss: 1.5879\n",
      "\n",
      "Fold 3:\n",
      "Resuming training from epoch 2/40\n",
      "  Epoch 3/40 — Train Loss: 39.9690, Val Loss: 35.4293\n",
      "  Epoch 4/40 — Train Loss: 31.8337, Val Loss: 14.7804\n",
      "  Epoch 5/40 — Train Loss: 23.6739, Val Loss: 14.9197\n",
      "  Epoch 6/40 — Train Loss: 16.9139, Val Loss: 7.6400\n",
      "  Epoch 7/40 — Train Loss: 11.8642, Val Loss: 6.3066\n",
      "  Epoch 8/40 — Train Loss: 9.7872, Val Loss: 5.8313\n",
      "  Epoch 9/40 — Train Loss: 7.6731, Val Loss: 4.6068\n",
      "  Epoch 10/40 — Train Loss: 6.4835, Val Loss: 2.9396\n",
      "  Epoch 11/40 — Train Loss: 4.3198, Val Loss: 2.7760\n",
      "  Epoch 12/40 — Train Loss: 4.0667, Val Loss: 2.7560\n",
      "  Epoch 13/40 — Train Loss: 3.9277, Val Loss: 2.7468\n",
      "  Epoch 14/40 — Train Loss: 3.4180, Val Loss: 3.4584\n",
      "  Epoch 15/40 — Train Loss: 3.4048, Val Loss: 3.7174\n",
      "  Epoch 16/40 — Train Loss: 3.7323, Val Loss: 3.2310\n",
      "  Epoch 17/40 — Train Loss: 2.7427, Val Loss: 2.3355\n",
      "  Epoch 18/40 — Train Loss: 1.9559, Val Loss: 2.3480\n",
      "  Epoch 19/40 — Train Loss: 1.7335, Val Loss: 1.6292\n",
      "  Epoch 20/40 — Train Loss: 1.7831, Val Loss: 2.0179\n",
      "  Epoch 21/40 — Train Loss: 2.1847, Val Loss: 1.9426\n",
      "  Epoch 22/40 — Train Loss: 1.8354, Val Loss: 1.8785\n",
      "  Epoch 23/40 — Train Loss: 1.8940, Val Loss: 2.6765\n",
      "  Epoch 24/40 — Train Loss: 1.7131, Val Loss: 2.2978\n",
      "  Epoch 25/40 — Train Loss: 1.7091, Val Loss: 1.9645\n",
      "  Epoch 26/40 — Train Loss: 1.7519, Val Loss: 2.0514\n",
      "  Epoch 27/40 — Train Loss: 1.6813, Val Loss: 1.3117\n",
      "  Epoch 28/40 — Train Loss: 1.4496, Val Loss: 2.3488\n",
      "  Epoch 29/40 — Train Loss: 1.7079, Val Loss: 1.6134\n",
      "  Epoch 30/40 — Train Loss: 1.4928, Val Loss: 1.5480\n",
      "  Epoch 31/40 — Train Loss: 1.0839, Val Loss: 1.4450\n",
      "  Epoch 32/40 — Train Loss: 1.2925, Val Loss: 1.5694\n",
      "  Epoch 33/40 — Train Loss: 0.9864, Val Loss: 1.3041\n",
      "  Epoch 34/40 — Train Loss: 0.9764, Val Loss: 1.7518\n",
      "  Epoch 35/40 — Train Loss: 1.0583, Val Loss: 1.6009\n",
      "  Epoch 36/40 — Train Loss: 1.4390, Val Loss: 1.4042\n",
      "  Epoch 37/40 — Train Loss: 1.2599, Val Loss: 1.4701\n",
      "  Epoch 38/40 — Train Loss: 1.2557, Val Loss: 1.4653\n",
      "  Epoch 39/40 — Train Loss: 1.4246, Val Loss: 1.6887\n",
      "  Epoch 40/40 — Train Loss: 1.8330, Val Loss: 3.9186\n",
      "\n",
      "Fold 4:\n",
      "Resuming training from epoch 2/40\n",
      "  Epoch 3/40 — Train Loss: 42.6226, Val Loss: 23.3613\n",
      "  Epoch 4/40 — Train Loss: 30.6009, Val Loss: 14.3314\n",
      "  Epoch 5/40 — Train Loss: 26.7292, Val Loss: 14.5045\n",
      "  Epoch 6/40 — Train Loss: 22.6187, Val Loss: 11.7667\n",
      "  Epoch 7/40 — Train Loss: 18.8691, Val Loss: 9.6703\n",
      "  Epoch 8/40 — Train Loss: 13.1549, Val Loss: 9.4037\n",
      "  Epoch 9/40 — Train Loss: 11.8980, Val Loss: 5.6367\n",
      "  Epoch 10/40 — Train Loss: 8.6021, Val Loss: 4.5385\n",
      "  Epoch 11/40 — Train Loss: 7.9093, Val Loss: 7.1920\n",
      "  Epoch 12/40 — Train Loss: 6.4851, Val Loss: 3.9292\n",
      "  Epoch 13/40 — Train Loss: 4.8365, Val Loss: 2.5637\n",
      "  Epoch 14/40 — Train Loss: 4.1739, Val Loss: 2.4572\n",
      "  Epoch 15/40 — Train Loss: 2.9472, Val Loss: 1.9487\n",
      "  Epoch 16/40 — Train Loss: 2.3645, Val Loss: 2.7951\n",
      "  Epoch 17/40 — Train Loss: 2.0958, Val Loss: 2.2630\n",
      "  Epoch 18/40 — Train Loss: 1.9310, Val Loss: 1.8437\n",
      "  Epoch 19/40 — Train Loss: 1.7323, Val Loss: 1.9215\n",
      "  Epoch 20/40 — Train Loss: 1.4946, Val Loss: 1.7488\n",
      "  Epoch 21/40 — Train Loss: 1.4053, Val Loss: 1.4715\n",
      "  Epoch 22/40 — Train Loss: 1.1371, Val Loss: 1.5852\n",
      "  Epoch 23/40 — Train Loss: 1.1240, Val Loss: 1.8225\n",
      "  Epoch 24/40 — Train Loss: 1.4712, Val Loss: 1.3533\n",
      "  Epoch 25/40 — Train Loss: 1.2200, Val Loss: 1.6927\n",
      "  Epoch 26/40 — Train Loss: 1.1287, Val Loss: 1.5513\n",
      "  Epoch 27/40 — Train Loss: 1.0914, Val Loss: 1.4136\n",
      "  Epoch 28/40 — Train Loss: 1.0936, Val Loss: 1.5084\n",
      "  Epoch 29/40 — Train Loss: 1.1220, Val Loss: 1.8668\n",
      "  Epoch 30/40 — Train Loss: 1.1830, Val Loss: 1.7298\n",
      "  Epoch 31/40 — Train Loss: 1.4069, Val Loss: 1.7903\n",
      "  Epoch 32/40 — Train Loss: 1.3463, Val Loss: 1.6134\n",
      "  Epoch 33/40 — Train Loss: 1.0373, Val Loss: 1.7131\n",
      "  Epoch 34/40 — Train Loss: 1.0833, Val Loss: 1.2922\n",
      "  Epoch 35/40 — Train Loss: 1.2088, Val Loss: 1.2867\n",
      "  Epoch 36/40 — Train Loss: 0.9810, Val Loss: 1.5075\n",
      "  Epoch 37/40 — Train Loss: 1.0327, Val Loss: 2.1594\n",
      "  Epoch 38/40 — Train Loss: 1.3766, Val Loss: 1.3996\n",
      "  Epoch 39/40 — Train Loss: 1.2578, Val Loss: 1.3128\n",
      "  Epoch 40/40 — Train Loss: 1.2263, Val Loss: 1.1676\n",
      "linear_regression: 1.574 ± 0.445\n",
      "linear_regression full\n",
      "\n",
      "Fold 0:\n",
      "Resuming training from epoch 2/40\n",
      "  Epoch 3/40 — Train Loss: 9297165.1000, Val Loss: 6957499.8750\n",
      "  Epoch 4/40 — Train Loss: 9024131.5000, Val Loss: 2850414.0000\n",
      "  Epoch 5/40 — Train Loss: 5137236.9000, Val Loss: 3104995.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m targets \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_targets.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m stats \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/daily_input_stats_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mrun_crossval\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/convective_outlook_ml/src/crossval.py:91\u001b[0m, in \u001b[0;36mrun_crossval\u001b[0;34m(X, y, model_name, n_splits, batch_size, epochs, optimizer_class, lr, criterion, level, restart)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ==== Training loop ====\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs):\n\u001b[0;32m---> 91\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, epoch, writer)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m — Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/convective_outlook_ml/src/train_loop.py:7\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion, epoch, writer)\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m running_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/convective_outlook_ml/src/dataset.py:22\u001b[0m, in \u001b[0;36mLazyWeatherDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m da \u001b[38;5;241m=\u001b[39m day_data[var]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m da\u001b[38;5;241m.\u001b[39mdims:\n\u001b[0;32m---> 22\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlevel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mstack(features\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtod\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140\u001b[0m, in \u001b[0;36mdeprecate_dims.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     emit_user_level_warning(\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` argument has been renamed to `dim`, and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the future. This renaming is taking place throughout xarray over the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old_name)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/core/dataarray.py:2881\u001b[0m, in \u001b[0;36mDataArray.stack\u001b[0;34m(self, dim, create_index, index_cls, **dim_kwargs)\u001b[0m\n\u001b[1;32m   2815\u001b[0m \u001b[38;5;129m@partial\u001b[39m(deprecate_dims, old_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdim_kwargs: Sequence[Hashable],\n\u001b[1;32m   2822\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03m    Stack any number of existing dimensions into a single new dimension.\u001b[39;00m\n\u001b[1;32m   2825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;124;03m    DataArray.unstack\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2881\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2885\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdim_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2886\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140\u001b[0m, in \u001b[0;36mdeprecate_dims.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     emit_user_level_warning(\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` argument has been renamed to `dim`, and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the future. This renaming is taking place throughout xarray over the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old_name)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/core/dataset.py:5359\u001b[0m, in \u001b[0;36mDataset.stack\u001b[0;34m(self, dim, create_index, index_cls, **dim_kwargs)\u001b[0m\n\u001b[1;32m   5357\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   5358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_dim, dims \u001b[38;5;129;01min\u001b[39;00m dim\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 5359\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stack_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/core/dataset.py:5274\u001b[0m, in \u001b[0;36mDataset._stack_once\u001b[0;34m(self, dims, new_dim, index_cls, create_index)\u001b[0m\n\u001b[1;32m   5272\u001b[0m shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msizes[d] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m vdims]\n\u001b[1;32m   5273\u001b[0m exp_var \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39mset_dims(vdims, shape)\n\u001b[0;32m-> 5274\u001b[0m stacked_var \u001b[38;5;241m=\u001b[39m \u001b[43mexp_var\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mnew_dim\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5275\u001b[0m new_variables[name] \u001b[38;5;241m=\u001b[39m stacked_var\n\u001b[1;32m   5276\u001b[0m stacked_var_names\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:140\u001b[0m, in \u001b[0;36mdeprecate_dims.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     emit_user_level_warning(\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` argument has been renamed to `dim`, and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the future. This renaming is taking place throughout xarray over the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old_name)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/core/variable.py:1446\u001b[0m, in \u001b[0;36mVariable.stack\u001b[0;34m(self, dim, **dim_kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_dim, dims \u001b[38;5;129;01min\u001b[39;00m dim\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1446\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stack_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/core/variable.py:1409\u001b[0m, in \u001b[0;36mVariable._stack_once\u001b[0;34m(self, dim, new_dim)\u001b[0m\n\u001b[1;32m   1406\u001b[0m reordered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m*\u001b[39mdim_order)\n\u001b[1;32m   1408\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m reordered\u001b[38;5;241m.\u001b[39mshape[: \u001b[38;5;28mlen\u001b[39m(other_dims)] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)\n\u001b[0;32m-> 1409\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mduck_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreordered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m new_dims \u001b[38;5;241m=\u001b[39m reordered\u001b[38;5;241m.\u001b[39mdims[: \u001b[38;5;28mlen\u001b[39m(other_dims)] \u001b[38;5;241m+\u001b[39m (new_dim,)\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[1;32m   1413\u001b[0m     new_dims, new_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoding, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m )\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:413\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(array, shape)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(array, shape):\n\u001b[1;32m    412\u001b[0m     xp \u001b[38;5;241m=\u001b[39m get_array_namespace(array)\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/numpy/core/fromnumeric.py:285\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/milesep/conda-envs/mlco/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compare all models, save training and validation curves along with corresponding model name, data level, model/training hyperparameters\n",
    "model_names = [\"linear_regression\"]\n",
    "levels = [\"small\"]\n",
    "# Don't do linear regression with full dataset--too many parameters\n",
    "for name, level in zip(model_names, levels):\n",
    "    print(name, level)\n",
    "    inputs = xr.open_zarr(f\"{input_dir}/train_inputs_{level}.zarr\")\n",
    "    targets = xr.open_dataset(f\"{target_dir}/train_targets.nc\")\n",
    "    stats = xr.open_dataset(f\"{stats_dir}/daily_input_stats_{level}.nc\")\n",
    "    scores = run_crossval(inputs, targets, name, epochs = 40, level = level, restart = False)\n",
    "    print(f\"{name}: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlco]",
   "language": "python",
   "name": "conda-env-mlco-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
